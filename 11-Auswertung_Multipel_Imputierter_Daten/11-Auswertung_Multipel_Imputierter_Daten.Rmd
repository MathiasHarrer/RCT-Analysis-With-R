---
title: Evaluation Randomisiert-Kontrollierter Studien und Experimente mit \textsf{R}
subtitle: "Auswertung Multipel Imputierter Daten: Die Rubin-Regeln"
author: Prof. Dr. David Ebert & Mathias Harrer
date: Graduiertenseminar TUM-FGZ
institute: Psychology & Digital Mental Health Care, Technische Universität München
output: binb::metropolis
fontsize: 11pt
bibliography: assets/bib.bib
csl: assets/apa.csl
monofont: "Fira Code"
mainfont: "Fira Sans"
header-includes: |
  ```{=latex}
  \usepackage{calc}
  \usepackage{mathspec}
  \usepackage{booktabs}
  \usepackage{makecell}
  \usepackage{amsmath,amsthm}
  \makeatletter
  \let\@@magyar@captionfix\relax
  \makeatother
  \usepackage{xcolor}
  \usepackage{tikz}
  \definecolor{protectBlue}{RGB}{48,124,148}
  \definecolor{protectGreen}{RGB}{161, 198, 66}
  \definecolor{verywhite}{rgb}{1, 1, 1}
  \usepackage{multicol}
  \hypersetup{colorlinks,citecolor=protectGreen,
    filecolor=red,linkcolor=protectGreen,urlcolor=blue}
  \setbeamercolor{progress bar}{fg=protectGreen}
  \setbeamercolor{alerted text}{fg=protectGreen}
  \setbeamercolor{frametitle}{bg=protectBlue}
  \setbeamercolor{normal text}{fg=protectBlue}
  \titlegraphic{%
  \hspace*{6cm}~%
  \includegraphics[width=2cm]{assets/logo/tum-lightblue}
  \hspace*{0.3cm}~%
  \includegraphics[width=2cm]{assets/logo/protect}}
  \setbeamertemplate{frame footer}{Evaluation Randomisiert-Kontrollierter Studien und Experimente mit \textsf{R}}%         <- !!SET FOOTER TITLE!!
  \makeatletter
  \setlength{\metropolis@frametitle@padding}{2.2ex}
  \setbeamertemplate{footline}{%
      \begin{beamercolorbox}[wd=\textwidth, sep=0.7ex]{footline}
          \usebeamerfont{page number in head/foot}%
          \usebeamertemplate*{frame footer}
          \hfill%
          \usebeamertemplate*{frame numbering}
      \end{beamercolorbox}%
  }
  \setbeamertemplate{frametitle}{%
    \nointerlineskip%
    \begin{beamercolorbox}[%
        wd=\paperwidth,%
        sep=0pt,%
        leftskip=\metropolis@frametitle@padding,%
        rightskip=\metropolis@frametitle@padding,%
      ]{frametitle}%
    \metropolis@frametitlestrut@start%
    \insertframetitle%
    \nolinebreak%
    \metropolis@frametitlestrut@end%
    \hfill
    \includegraphics[height=2ex,keepaspectratio]{assets/logo/tum_white}
    \end{beamercolorbox}%
  }
  \newlength{\cslhangindent}
  \setlength{\cslhangindent}{1.5em}
  \newenvironment{CSLReferences}[3][0]%
  {\setlength{\parindent}{0pt}%
  \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces}%
  ```
---

```{r,setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, comment = "#>", background = '#F7F7F7')
```


# Die 3 Variationsquellen

## Die 3 Variationsquellen (I)

Wie komme ich mit $m$ Imputationssets zu **\underline{einem} gemeinsamen Analyseergebnis**?

\includegraphics[width=1\textwidth]{assets/mi2.png}

\scriptsize adaptiert von @van2011mice. \par \normalsize


## Die 3 Variationsquellen (II)

\small

$\rightarrow$ **\alert{Lösung}**: Durchführung der selben Analyse in allen $m$ imputierten Datensets
parallel, dann werden alle $m$ relevanten Parameter/Schätzwerte zu einem Wert aggregiert (**"gepoolt"**).

Dabei müssen wir miteinbeziehen, dass unsere Schätzwerte **mit Unsicherheit behaftet** sind. Diese Unsicherheit hat bei MI mindestens zwei Gründe:

1. Die Teilnehmenden der Studie stellen nur eine Stichprobe der untersuchten Studienpopulation dar, sind also mit Stichprobenfehler (**"sampling error"**) behaftet.

2. Die Daten enthalten **fehlende Werte**, deren Schätzung **unsicher** ist. Diese Unsicherheit wird dadurch reflektiert, dass multipel imputierte Werte sich zwischen Imputationssets unterscheiden (können).

\par \normalsize


## Die 3 Variationsquellen (III)

\small

Es sei $Q$ ein zu **schätzender wahrer Wert** (oder ein Vektor von Werten) der Population (z.B. Populationsmittelwert, Regressionskoeffizienten, ...). 

Aufgrund der zuvor genannten Gründe ist $Q$ **unbekannt** und muss durch einen Schätzer $\hat{Q}$ **angenähert** werden. Dies ist nur durch die **beobachteten Werte** $Y_{\text{obs}}$ möglich.

Der Erwartungswert (d.h. die **bestmögliche Annäherung**) von $Q$ gegeben $Y_{\text{obs}}$ ist [@vanbuuren, Kapitel 2.3.2]:

$$E(Q|Y_{\text{obs}})=E\bigg(E(Q|Y_{\text{obs}}, Y_{\text{mis}})~\bigg|~Y_{\text{obs}}\bigg)$$
d.h. der Durchschnittswert der (imputierten) Schätzungen des Mittelwerts von $Q$ über alle multiplen Imputationen hinweg.

\par \normalsize

## Die 3 Variationsquellen (IV)

**Kombination von Punktschätzungen**:


\small

Es sei $\hat{Q}_{\ell}$ die Schätzung von $Q$ im $\ell$-ten von $m$ Imputationssets. Der "gepoolte" Schätzwert von $Q$ ist damit:


$$\bar Q = \frac{1}{m}\sum_{\ell=1}^m \hat Q_\ell$$

\metroset{block=fill} 

\begin{exampleblock}{Aggregation von Punktschätzungen bei MI}
  Um Punktschätzungen (z.B. Parameter der Wahrscheinlichkeitsverteilung wie Mittelwert \& Standardabweichung; Regressionsgewichte, etc.) zu poolen, wird in \textbf{jedem Imputationsset} der Wert des Punktschätzers \textbf{berechnet}, und daraufhin der \textbf{Mittelwert über alle Imputationsets} gebildet.
\end{exampleblock}

\par \normalsize


## Die 3 Variationsquellen (V)

\small

Aber wie kann die **_Unsicherheit_** (Varianz) von $Q$ in MI-Daten geschätzt werden? Die Varianz von $Q$ gegeben $Y_{\text{obs}}$ besteht aus **zwei Komponenten**:

$$V(Q|Y_{\text{obs}}) = \underbrace{E\bigg(V(Q|Y_{\text{obs}}, Y_{\text{mis}})~ \bigg| ~ Y_{\text{obs}} \bigg)}_{\scriptsize \begin{split} &\text{Mittelwert d. Varianzen über alle MI-Sets} \\ & \alert{\rightarrow \textbf{Within-Variance}~(\bar{{U}})} \end{split}} + \underbrace{V\bigg( E(Q|Y_{\text{obs}}, Y_{\text{mis}})~\bigg|~ Y_{\text{obs}}\bigg)}_{\scriptsize \begin{split} &\text{Varianz d. Mittelwerte über alle MI-Sets} \\ & \alert{\rightarrow \textbf{Between-Variance}~(B)} \end{split}}$$
\metroset{block=fill} 

\begin{exampleblock}{Bestimmung der gepoolten Varianz von Parametern bei MI}
  Um die Varianz eines Parameters $Q$ zu bestimmen (z.B. für Konfidenzintervalle), muss bei MI sowohl die (gemittelte) Varianz durch den Stichprobenfehler $\bar{U}$, als auch die Imputationsunsicherheit $B$ mit berücksichtigt werden. Die Berechnung der gepoolten Varianz erfolgt durch die sog. \textbf{"Rubin-Regeln"} (s. n. Folie). 
\end{exampleblock}

\par \normalsize


## Die 3 Variationsquellen (VI)

**\alert{"Rubin's Rules" - Die Kombinationsregeln nach Rubin}** \scriptsize [@rubin1987multiple] \par \normalsize

\small Die Rubin-Regeln stellen eine allgemeine Formel dar, nach der die MI-Varianz eines 
Punktschätzers $Q$ im konkreten Fall berechnet werden kann: \par \normalsize

\begin{align*}
\openup 3\jot
\begin{split}
\hat{V} & = \overbrace{ \left(\frac{1}{m}\sum^{m}_{\ell=1}\bar{U}_\ell\right)}^{\bar{U}} + \left(1+\frac{1}{m}\right) \overbrace{ \left(\frac{1}{m-1}\sum_{\ell=1}^m (\hat Q_\ell-\bar Q)(\hat Q_\ell-\bar Q)'\right)}^{B} \\
& = \bar{U} + \left(1+\frac{1}{m}\right)B \\
& \Rightarrow \bar{U} + B ~~~~~\text{as}~~~~~m \rightarrow \infty
\end{split}
\end{align*}

## Die 3 Variationsquellen (VII)

\begin{center}

\textbf{...aber warum \emph{drei} Variationsquellen?}

\includegraphics[width=0.7\textwidth]{assets/third-kind.png}

\end{center}

## Die 3 Variationsquellen (VIII)

Die Rubin-Formeln beziehen auch mit ein, dass immer nur eine finite Anzahl an Imputationssets generiert werden ($\rightarrow$ Einbezug der **\alert{Simulationsvarianz}**).

\begin{align*}
\openup 3\jot
\begin{split}
\hat{V} & = \overbrace{ \left(\frac{1}{m}\sum^{m}_{\ell=1}\bar{U}_\ell\right)}^{\bar{U}} + \alert{\left(1+\frac{1}{m}\right) }\overbrace{ \left(\frac{1}{m-1}\sum_{\ell=1}^m (\hat Q_\ell-\bar Q)(\hat Q_\ell-\bar Q)'\right)}^{B} \\
& = \bar{U} + \alert{\left(1+\frac{1}{m}\right)}B \\
& \Rightarrow \bar{U} + B ~~~~~\text{as}~~~~~m \rightarrow \infty
\end{split}
\end{align*}



## Die 3 Variationsquellen (IX) {.t}

$\rightarrow$ Je größer $m$, desto **geringer fällt diese Komponente ins Gewicht**. Insbesondere, wenn \underline{besonders} genaue Varianzschätzungen notwendig sind, empfiehlt sich daher eine **hohe Anzahl an Imputationssets**. \linebreak


Ein niedriges $m$ führt zu **Konfidenzintervallen**, die **\alert{etwas breiter}** sind als wenn $m \rightarrow \infty$ (niedrigere Effizienz). Dieser Unterschied ist in der Praxis jedoch typischerweise **überschaubar**.


# Metriken & Freiheitsgrade bei MI-Analysen

## Metriken & Freiheitsgrade bei MI-Analysen (I)

**\alert{Metriken für Parameterschätzungen in MI}** \scriptsize [@vanbuuren, Kapitel 2.3.5] \par \normalsize

\small

**_Relative Increase in Variance Due to Nonresponse_** (RIV): Relativer Anstieg der Varianz aufgrund der Imputationsunsicherheit (wenn RIV > 1: Imputationsvarianz größer als "echte" Varianz in $Y$):

$$r_Q = \frac{B_Q/m +B_Q}{\bar{U}_Q} $$
**_Fraction of Missing Information Due to Nonresponse_** (FMI): Anteil der Information über $Q$, die durch die Imputationsunsicherheit "verloren geht":

$$\gamma_Q = \frac{(r_Q+2)/(\nu_Q+3)}{1+r_Q} $$

Wobei $\nu$ ("nu") für die Freiheitsgrade bei der Schätzung von $Q$ steht. \par \normalsize

## Metriken & Freiheitsgrade bei MI-Analysen (II) {.t}

**\alert{Freiheitsgrade bei MI-Analysen}** \scriptsize [@vanbuuren, Kapitel 2.3.6] \par \normalsize

\small
Die Freiheitsgrade eines Modells sind definiert als **Anzahl der Beobachtungen** nach Abzug der **Modellparameter**: $\nu = n - k$ (für Mittelwert z.B. $\nu = n-1$).

Bei MI sind manche Elemente von $n$ nur **Schätzungen von Beobachtungen**, daher muss $\nu$ dafür **korrigiert** werden [@rubin1987multiple]:


$$\nu_{\text{(MI)}} = (m-1)\left(1+ \frac{1}{r^2}\right)$$
$$\lim_{r \rightarrow 0}~\nu_{\text{(MI)}} = \infty~~~~~~\text{sowie}~~~~~~\lim_{r \rightarrow \infty} \nu_{\text{(MI)}} = m-1$$

Diese Formel basiert auf der Annahme, dass die **Freiheitsgrade des vollständigen Datensatzes** (den MI zu schätzen versucht) **unendlich groß** sind! Diese Approximation ist aber erst sinnvoll, wenn ein **relativ großes Sample** vorliegt. \par \normalsize

## Metriken & Freiheitsgrade bei MI-Analysen (III) {.t}

**\alert{Freiheitsgrade bei MI-Analysen}** \scriptsize [@vanbuuren, Kapitel 2.3.6] \par \footnotesize

Für kleine Stichproben kann eine adaptierte Formel genutzt werden [@barnard1999miscellanea]. Dafür müssen zuerst die **"hypothetischen" Freiheitsgrade** bestimmt werden, wenn die Daten komplett wären ("complete data degress of freedom; $\nu_{\text{com}}$). Es sei $n$ die Anzahl der Beobachtungen und $k$ die Anzahl der Parameter:


$$\nu_{\text{com}} = n-k $$
Daraus lassen sich die Freiheitsgrade der **beobachteten Werte** ($\nu_{\text{obs}}$) bestimmen:

$$\nu_\text{obs} = \frac{\nu_\text{com}+1}{\nu_\text{com}+3}\nu_\text{com}\left(1-\frac{r}{r+1}\right)$$

Diese können wiederum zur **Korrektur** von $\nu_{\text{(MI)}}$ verwendet werden:

$$\nu^*_{\text{(MI)}} =
  \frac{\nu_{\text{(MI)}} \nu_\text{obs}}
  {\nu_{\text{(MI)}}+\nu_\text{obs}}$$


\par \normalsize

## Metriken & Freiheitsgrade bei MI-Analysen (IV) {.t}

**\alert{Freiheitsgrade bei MI-Analysen}** \scriptsize [@vanbuuren, Kapitel 2.3.6] \par \normalsize

**Praktische Anmerkungen**

- Wird die unkorrigierte Formel verwendet (default bei _{mitml}_), können die **angezeigten Freiheitsgrade** eines Modells/Parameters **größer als $n$ sein**!

- Da die Anzahl der Freiheitsgrade bei MI anhand der obigen Formeln angenähert wird, muss $\nu$ im konkreten Fall **\underline{keine} natürliche Zahl** sein (z.B. $\nu$ = 226.6559)!


## Metriken & Freiheitsgrade bei MI-Analysen (V) {.t}


**\alert{Konfidenzintervalle}** \scriptsize [@vanbuuren, Kapitel 2.4.2] \par \footnotesize

Die berechneten Freiheitsgrade können zur Berechnung von **Konfidenzintervallen** genutzt werden. Zur Inferenz von Skalaren (= $\bar{Q}$ ist ein einziger Wert) wird dabei häufig eine $t$-Verteilung angenommen. 

$\bar{Q}$ sei ein aggregierter Parameter (z.B. ein Regressionsgewicht $b$), und $Q_0$ der Referenzwert der Nullhypothese (typischerweise 0):

$$\frac{\bar{Q}-Q_0}{\sqrt{\hat{V}}} \sim t_\nu $$

Zusammen mit den berechneten MI-Freiheitsgraden lässt sich so ein passendes 95% Konfidenzintervall berechnen, dass die Imputationsunsicherheit berücksichtigt:

$$\bar{Q}~\pm~t_{\nu^{(*)}_{\text{(MI)}}, \text{0.975}} \times \sqrt{\hat{V}}$$

\par \normalsize


# Wichtige Anmerkungen zur Analyse von MI-Daten

## Wichtige Anmerkungen zur Analyse von MI-Daten (I) {.t}

\footnotesize

\metroset{block=fill} 

\begin{exampleblock}{Cave I: Aggregation von Teststatistiken}
  \begin{itemize}
    \item In der Praxis ist es häufig notwendig, Teststatistiken (z.B. $\chi^2$ oder $F$-Werte) in \textbf{jedem Imputationsset zu berechnen}, und \textbf{daraufhin zu poolen}.
    \item Teststatistiken können aber typischerweise nicht einfach wie bei $\bar{Q}$ durch das \textbf{arithmetische Mittel} gepoolt werden!
    \item Hintergrund dafür ist, dass die \textbf{Größe der Teststatistiken} von \textbf{Varianz} \& \textbf{Freiheitsgraden} abhängig ist. Diese ist/sind \textbf{bei MI größer bzw. kleiner} aufgrund der \textbf{Imputationsunsicherheit}.
    \item Um Teststatistiken nicht zu \textbf{überschätzen}, müssen daher \textbf{besondere Formeln} zur Aggregation eingesetzt werden. 
    \item Implementationen in R sind beispielsweise die \texttt{micombine.chisquare} und \texttt{micombine.F} function im \emph{\{miceadds\}} package.
  \end{itemize}
\end{exampleblock}

\par \scriptsize

$\rightarrow$ Methoden zur Aggregation von Test(statistiken) sind ein **aktives Forschungsfeld**, und weitere Implementierungen in R sind zu erwarten [s. z.B. @grund2021pooling].

\par \normalsize

## Wichtige Anmerkungen zur Analyse von MI-Daten (II) {.t}

\footnotesize

\metroset{block=fill} 

\begin{exampleblock}{Cave II: Aggregation von Korrelationen}
  \begin{itemize}
    \item Der Wert einer Korrelation $\rho$ kann die \textbf{Maximalwerte} $[-1;1]$ \textbf{nicht überschreiten}; dies bedeutet, dass die \textbf{Varianz} von $\rho$ \textbf{eingeschränkt} wird, je weiter $|\rho|$ gegen 1 geht. 
    \item Dies führt dazu, dass für $\rho$ \textbf{keine asymptomtische Normalverteilung} angenommen werden kann.
    \item Dadurch kann zur Aggregation von Korrelationen \textbf{\underline{nicht} einfach der Mittelwert} berechnet werden. Korrelationen sollten vorher einer \textbf{varianzstabilisierenden Transformation} unterzogen werden, der \textbf{Fisher $z$-Transformation}. 
    \item In R können Korrelationen komfortabel mit der \texttt{micombine.cor} function im \emph{\{miceadds\}} package aggregiert werden.
  \end{itemize}
\end{exampleblock}

vgl. @marshall2010comparison. 

\par \normalsize


## Wichtige Anmerkungen zur Analyse von MI-Daten (III) {.t}

\footnotesize

\metroset{block=fill} 

\begin{exampleblock}{Cave III: Analyse in aggregierten Daten}
  \begin{itemize}
    \item In der Praxis findet man häufig das Vorgehen, \textbf{alle MI-Sets} zu \textbf{\underline{einem} vollständigen Datensatz} zu aggregieren, und dann Analysen in diesem (einen) Datensatz durchzuführen.
    \item Dies erleichtert zwar die Auswertung mit gängiger Statistiksoftware enorm, ist aber aus \textbf{statistischer Sicht \underline{unbedingt} zu vermeiden}!
    \item Durch die Aggregation zu einem Datenset wird \textbf{"vorgegaukelt", dass keine Imputationsunsicherheit existiert}; dies führt zu inkorrekten, weil antikonservativen $p$-Werten, Konfidenzintervallen, Teststatistiken, etc.   
    \item Eine Analyse in aggregierten Daten sollte daher höchstens dann durchgeführt werden, wenn keinerlei statistische Tests oder Quantifizierung der Parameterunsicherheit angestrebt wird; dies ist jedoch in der Praxis selten der Fall.
  \end{itemize}
\end{exampleblock}

vgl. @vanbuuren, Kapitel 5.1.2.


\par \normalsize


## "Rinse & Repeat": Parallele Bearbeitung von Listenelementen

\begin{tikzpicture}[remember picture,overlay]  
  \node [xshift=0cm,yshift=-0.25cm] at (current page.center)
    {\includegraphics[height=7cm]{assets/loop.png}};
\end{tikzpicture}

## "Rinse & Repeat": Parallele Bearbeitung von Listenelementen (I)

\small

Bei der Auswertung von multipel imputierten Daten in R müssen häufig **Operationen in allen $m$ Sets gleichzeitig durchgeführt** werden. Dies führt oft dazu, dass selbst einfache Analyseschritte **deutlich komplizierter** werden.

**\alert{Eine mögliche Implementierung sind \texttt{for} Loops:}**


```{r, eval=FALSE, comment="##>", background="green"}
# Berechne aggregierten Mittelwert über alle Sets
means = vector()
for (i in 1:25){
  means[[i]] <- implist[[i]] %>% pull(pss.0) %>% mean()
}
mean <- mean(means)

```

$\rightarrow$ **Nachteil**: trotz einfacher Operation komplexer Code, lange Rechenzeit.

\par \normalsize

## "Rinse & Repeat": Parallele Bearbeitung von Listenelementen (II) {.t}

**\alert{Functional Programming}** \scriptsize [@wickham2019advanced, Kapitel 9] \par \normalsize

\small
Anstatt mit dem gleichen Befehl durch alle Sets zu loopen, kann stattdessen ein **"funktionaler" Programmierstil** gewählt werden. D.h. es wird eine **Funktion** genutzt, die **wiederum selbst Funktionen** auf alle Imputationssets **anwendet**.

In Base-R sind dies Funktionen wie `apply`, `mapply`, `vapply`, `Reduce`, etc. Besonders benutzerfreundlich und konsistent sind aber die `map`-Funktionen im package _{purrr}_ [@purrr]: 


```{r, eval=FALSE, comment="##>", background="green"}
mean <- implist %>% 
          map_dbl(~mean(.$pss.0)) %>% 
          mean()
```

$\rightarrow$ **Vorteil**: knapper, übersichtlicher Code; stark verkürzte Rechenzeit. 

\begin{tikzpicture}[remember picture,overlay]  
  \node [xshift=-1.2cm,yshift=-8.2cm] at (current page.north east)
    {\includegraphics[height=1.7cm]{assets/purrr.png}};
\end{tikzpicture}

## "Rinse & Repeat": Parallele Bearbeitung von Listenelementen (III) {.t}

**\alert{Functional Programming}** \scriptsize [@wickham2019advanced, Kapitel 9] \par \normalsize

\small
**Die Funktionsweise von `map`:** \par \normalsize

\begin{center}
\includegraphics[width=0.7\textwidth]{assets/map.png}
\end{center}


\begin{tikzpicture}[remember picture,overlay]  
  \node [xshift=-1.2cm,yshift=-8.2cm] at (current page.north east)
    {\includegraphics[height=1.7cm]{assets/purrr.png}};
\end{tikzpicture}

## "Rinse & Repeat": Parallele Bearbeitung von Listenelementen (IV) {.t}

**\alert{Functional Programming}** \scriptsize [@wickham2019advanced, Kapitel 9] \par \normalsize

\small
**Die "Geschmacksrichtungen" von `map`:**

- **`map`**: Output ist ein `list`-Objekt.

- **`map_dbl`**: Output ist ein `numeric`-Vektor.

- **`map_chr`**: Output ist ein `character`-Vektor.

- **`map_lgl`**: Output ist ein `logical`-Vektor.

- **`map_dfr`**: Output ist ein `data.frame`.

- **`map2(_*)`**: Iteration über zwei Listen gleichzeitig. 

\begin{tikzpicture}[remember picture,overlay]  
  \node [xshift=-1.2cm,yshift=-8.2cm] at (current page.north east)
    {\includegraphics[height=1.7cm]{assets/purrr.png}};
\end{tikzpicture}

\par \normalsize

## "Rinse & Repeat": Parallele Bearbeitung von Listenelementen (IV) {.t}

**\alert{Functional Programming}** \scriptsize [@wickham2019advanced, Kapitel 9] \par \normalsize

\small
**`map` akzeptiert Funktionen auf 2 Arten:**

1. **_"Klassisch"_**: eine "voll funktionstüchtige" Funktion wird in `map` gesteckt.

\footnotesize

```{r, eval=FALSE, comment="##>", background="green"}
# 'x' repräsentiert das individuelle Listenelement in 'list'
list %>% map(function(x) mean(x$variable))
```

\par \small

2. **_"Verkürzt"_**: mit "`~`" und "`.`" wird der Funktionscode abgekürzt.

\footnotesize

```{r, eval=FALSE, comment="##>", background="green"}
# '.' repräsentiert das individuelle Listenelement in 'list'
# Der Beginn einer Funktion wird durch '~' (tilde) angezeigt.
list %>% map(~ mean(.$variable))
```

\par \normalsize


\begin{tikzpicture}[remember picture,overlay]  
  \node [xshift=-1.2cm,yshift=-8.2cm] at (current page.north east)
    {\includegraphics[height=1.7cm]{assets/purrr.png}};
\end{tikzpicture}

## $~$

\begin{tikzpicture}[remember picture,overlay]  
  \node [xshift=0cm,yshift=-0.6cm] at (current page.center)
    {\includegraphics[width=1.05\paperwidth]{assets/bg-praxis.jpg}};
\end{tikzpicture}

\Huge
\colorbox{white}{\textbf{ Praxis-Teil }} \par \normalsize 

$$~$$
$$~$$

$$~$$

$$~$$

$$~$$

# Primäre Wirksamkeitsanalyse

## Primäre Wirksamkeitsanalyse (I)

\small

In der primären Wirksamkeitsanalyse wird die **Effektivität der Interventionsbedingung** evaluiert ("hatte die Interventionen einen Effekt?").

Die Analyse fokussiert dabei auf den **primären Endpunkt** (mit _a priori_ definiertem Messzeitpunkt und Messinstrument), und ob darin **Unterschiede zwischen den Gruppen** bestehen:


\begin{align*}
\begin{split}
|Y_{i,t} (X_1) - Y_{i,t}(X_0)| &> 0\\
\Rightarrow |\hat{\mu}_{1,t} - \hat{\mu}_{0,t}| &> 0
\end{split}
\end{align*}


Können wir dies statistisch Nachweisen, kann geschlossen werden, dass ein Effekt der Intervention vorliegt ($|\tau| > 0$).

Das übliche Verfahren hierzu stellt die **\alert{\emph{Analysis of Covariance} (ANCOVA)}** dar.

\par \normalsize


## Primäre Wirksamkeitsanalyse (II) {.t}

**\alert{Analysis of Covariance}** \scriptsize [@montgomery, Kapitel 15.3; @dunn2018generalized, Kapitel 2.9] \par \normalsize


\begin{multicols}{2}
  \footnotesize 
  ANCOVAs untersuchen, ob zwei oder mehrere Gruppen sich hinsichtlich einer gemessenen Variable $y$ unterscheiden, wenn für den \textbf{Einfluss} von (einer oder mehreren) \textbf{Kovariaten} (z.B. Baseline-Messung des primären Endpunkt) kontrolliert wird.

Die Methode der AN(C)OVA geht auf R.A. Fisher zurück, und ist eng mit der experimentellen Methodik randomisierter Studien verbunden. 

$$~$$
  \par \normalsize
\columnbreak
  \begin{center}
  \includegraphics[width=.5\textwidth]{assets/smrw.png}
  \newline \scriptsize  \emph{Statistical Methods for Research Workers} (1925) 
  \end{center}
  \par \normalsize
\end{multicols}



## Primäre Wirksamkeitsanalyse (III) {.t}

**\alert{Analysis of Covariance}** \scriptsize [@montgomery, Kapitel 15.3; @dunn2018generalized, Kapitel 2.9] \par \normalsize

\small 

$\rightarrow$ Die Durchführung von ANCOVAs in RCT-Analysen ist daher auch historisch zu erklären. Konzeptuell ist das Modell hinter ANCOVAs schlicht ein \textbf{Spezialfall linearer Regression!}

\par \normalsize

\begin{center}
  \includegraphics[width=.5\textwidth]{assets/regression.png}
\end{center}


## Primäre Wirksamkeitsanalyse (IV)

**\alert{Analysis of Covariance}** \scriptsize [@montgomery, Kapitel 15.3; @dunn2018generalized, Kapitel 2.9] \par \normalsize

\scriptsize

Es sei:

- $y_{ij}$ der Wert des (kontinuierlichen) primären Endpunkts von Person $j$ in Gruppe $i$;
- $\tau_i$ der Effekt der $i$-ten Behandlungsgruppe (z.B. Intervention oder Kontrolle);
- $\x_{ij}$ der Wert von $ij$ auf einer Kovariate.

**\underline{Das Modell der ANCOVA ist dann:}**

$$y_{ij} = \mu +\tau_i + \beta(x_{ij} - \bar{x}) + \epsilon_{ij} 
~~~\begin{cases}
      i = 1, 2, \dots a\\
      j = 1, 2, \dots n
    \end{cases}$$

Die **Residuale** des Modells folgen einer Normalverteilung mit Mittelwert 0: $\epsilon \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0,\sigma^2)$. ANCOVAs basieren auf einer **Effekt-Kodierung** der Treatment-Variable (z.B. -1 und 1), sodass sich die $\tau_i$-Werte zu null aufsummieren: 


$$\sum_{i=1}^{a} \tau_i = 0$$
\par \normalsize


## Primäre Wirksamkeitsanalyse (V)

**\alert{Analysis of Covariance}** \scriptsize [@montgomery, Kapitel 15.3; @dunn2018generalized, Kapitel 2.9] \par \normalsize

\small

AN(C)OVAs basieren auf dem Prinzip der **Varianzzerlegung**:

\begin{align*}
\begin{split}
\text{SS}_{\text{Total}} &= \text{SS}_{\text{Prädiktor}} + \text{SS}_{\text{Residual}} \\
\text{Daten} &= \text{Modellfit} + \text{Unerklärte Varianz}
\end{split}
\end{align*}



\scriptsize
\begin{center}
\renewcommand{\arraystretch}{1.5}
```{r, echo=F, message=FALSE, warning=FALSE}
library(kableExtra)
df = data.frame(
  Variationsquellen = c("Systematisch", 
                        "Zufällig", "Total"),
  Quadratsumme = c("$\\text{SS}_{\\text{Prädiktor}}$",
                     "$\\text{SS}_{\\text{Residual}}$", "$\\text{SS}_{\\text{Total}}$"),
  nu = c("$\\nu_{\\text{num}} = p-1$", 
         "$\\nu_{\\text{den}} = n-p$", "$n-1$"),
  MQ = c("$\\text{MSS}_{\\text{Prädiktor}} = \\frac{\\text{SS}_{\\text{Prädiktor}}}{p-1}$",
                              "$\\text{SS}_{\\text{Residual}} = \\frac{\\text{SS}_{\\text{Residual}}}{n-p}$", "-"))

colnames(df) = c("Variationsquelle", "Quadratsumme", "Freiheitsgrade", "Mittlere Quadratsumme")

knitr::kable(df, booktabs = TRUE, escape = FALSE) %>% 
  row_spec(0, bold = TRUE) %>% 
  row_spec(2, hline_after = TRUE) %>% 
  row_spec(3, hline_after = TRUE)
```
\renewcommand{\arraystretch}{1}
\par \normalsize
\end{center}
\scriptsize
$\rightarrow$ Bei ANCOVAs wird der Einfluss der Kovariate auf die Within-Varianz ($\text{SS}_{\text{Residual}}$) **"herausregerechnet"**.
\par \normalsize
\small


## Primäre Wirksamkeitsanalyse (VI) {.t}


**\alert{Analysis of Covariance}** \scriptsize [@montgomery, Kapitel 15.3; @dunn2018generalized, Kapitel 2.9] \par \normalsize

\small

Die **Signifikanz des Treatment-Effekts** wird dann über den $F$-Test ermittelt. Dieser vergleicht die **Variation durch den Treatmentfaktor** mit der **unerklärten Variation** in den Daten:

$$F_{\nu_{\text{num}},~\nu_{\text{den}}} = \frac{\text{SS}_{\text{Prädiktor}}/(p-1)}{\alert{\text{SS}_{\text{Residual}}}/(n-\alert{p})}$$

\metroset{block=fill} 

\begin{block}{Warum adjustieren? Reduktion von $\text{SS}_{\text{Residual}}$ bei ANCOVAs}
\scriptsize
\begin{itemize}
\itemsep-1em 
\item Durch den Einschluss prognostischer Variablen wird die unerklärte Varianz innerhalb der Gruppen verringert. Dadurch verkleinert sich $\text{SS}_{\text{Residual}}$ und der $F$-Wert wird größer $\Rightarrow$ mehr Power zum Nachweis des Treatment-Effekts! \linebreak
\item \textbf{Cave:} Dies ist nur er Fall, wenn die Kovariate tatsächlich prognostisch relevant ist. Wenn nicht, erhöht sich nur die Anzahl der Parameter $p$ $\Rightarrow$ \textbf{Power sinkt}!
\end{itemize}
\par \normalsize
\end{block}

## Primäre Wirksamkeitsanalyse (VII) {.t}


**\alert{Analysis of Covariance}** \scriptsize [@montgomery, Kapitel 15.3; @dunn2018generalized, Kapitel 2.9] \par \normalsize
\small

\underline{Vorteile der Adjustierung von Baseline-Variablen:}

- **Interpretierbarkeit**: _"The central question is for two patients with the same pre measurement value of $x$, one given treatment A and the other treatment B, will the patients tend to have different post-treatment values? This is exactly what analysis of covariance assesses."_ [@harrell1]

- **Power**: Adjustierung von prognostischen Variablen führt zu höherer Effizienz der Analysen (engere Konfidenzintervalle); bei dichotomen Outcomes (Odds Ratio) erhöht sich der Effekt selbst [@hernandez2004covariate].

- **Baselineunterschiede**: Kovariaten adjustieren für systematische Baselineunterschiede, sollten diese tatsächlich vorliegen.

\par \normalsize

## Primäre Wirksamkeitsanalyse (VIII) {.t}


**\alert{Analysis of Covariance}** \scriptsize [@montgomery, Kapitel 15.3; @dunn2018generalized, Kapitel 2.9] \par \normalsize
\small

\underline{Empfehlungen}

- Generell sollten nur Kovariation adjustiert werden, für die ein **starker prognostischer Zusammenhang plausibel** ist $\rightarrow$ bei der Baselinemessung des primären Outcomes voraussetzbar!

- Bei stratifizierter Randomisierung sollten die **Stratifizierungsvariablen** kontrolliert werden [@Kahane5840].

- Alle Kovariaten sollten **_a priori_** präspezifiert werden [@assmann2000subgroup].


## Primäre Wirksamkeitsanalyse (IX) {.t}

**\alert{Analysis of Covariance}** \scriptsize [@montgomery, Kapitel 15.3; @dunn2018generalized, Kapitel 2.9] \par \normalsize
\small

\underline{Anzahl der Kovariaten: Richtlinien der European Medicines Agency}

> _"No more than **a few covariates should be included** in the primary analysis. Even though methods of adjustment, such as analysis of covariance, can theoretically adjust for a large number of covariates it is safer to pre-specify a simple model."_

> _"Results based on such a model are more likely to be numerically stable, the assumptions underpinning the statistical model are easier to validate and generalisability of the results may be improved."_

@ema, Absatz 6.2.

\begin{tikzpicture}[remember picture,overlay]  
  \node [xshift=-2.3cm,yshift=-8.2cm] at (current page.north east)
    {\includegraphics[height=1.7cm]{assets/ema.jpg}};
\end{tikzpicture}

\par \normalsize

## $~$

\begin{tikzpicture}[remember picture,overlay]  
  \node [xshift=0cm,yshift=-0.6cm] at (current page.center)
    {\includegraphics[width=1.05\paperwidth]{assets/bg-praxis.jpg}};
\end{tikzpicture}

\Huge
\colorbox{white}{\textbf{ Praxis-Teil }} \par \normalsize 

$$~$$
$$~$$

$$~$$

$$~$$

$$~$$

# ANCOVAs in R

## ANCOVAs in R (I)

ANCOVAs basieren auf einem **linearen Modell**. Daher kann zur Berechnung als erster Schritt die **`lm`**-Funktion genutzt werden:

\underline{Notwendige Argumente für \texttt{lm} sind:}

* **\alert{\texttt{formula}}**: Die Formel des linearen Modells, mit der Grundform \linebreak "**`y ~ 1 + x`**". Dabei steht "`1`" für das Intercept (nicht zwingend notwendig). 
  - Interaktionen werden mit "`*`" gebildet, z.B. "`x1 * x2`".
  - Kontinuierliche Prädiktoren können mit `scale()` zentriert und skaliert werden, z.B. "`y ~ 1 + scale(x)`".

+ **\alert{\texttt{data}}**: Ein `data.frame` mit den zu verwendenden Daten.

$\rightarrow$ Eine **Varianzanalyse** des Modells kann dann durch den **\texttt{anova}**-Befehl durchgeführt werden.


## ANCOVAs in R (II)

**\alert{Berechnung in multipel imputierten Daten}**

\small

Mit Funktionen des **_{mitml}_ package** [@mitml] lassen sich (generalisierte) lineare Modelle unkompliziert in multipel imputierten Daten fitten.

Dazu müssen die Imputationen als eine Liste vom Typus "`mitml.list`" vorliegen. Diese Klasse kann durch die "`as.mitml.list`"-Funktion festgelegt werden.

Modelle können dann mit der `with`-Funktion **in allen MI-Sets gerechnet** werden, z.B.:

```{r, eval=F}
m <- with(implist, lm(y ~ 1 + trt + x))
```


Daraufhin kann die `testEstimates`-Funktion genutzt werden, um die **Modellparameter** mit den Rubin-Regeln zu **poolen**:

```{r, eval=F}
testEstimates(m)
```



\par \normalsize


# Klinische Signifikante & Reliable Veränderung

## Klinische Signifikante & Reliable Veränderung (I)

- Methoden wie AN(C)OVAs evaluieren nur, ob das **Treatment einen signifikanten Einfluss** auf den primären Endpunkt hat. 

- Dies sagt aber noch wenig darüber aus (1) wie **_bedeutsam_** dieser Einfluss ist, noch (2) was dieser Effekt auf **individueller Ebene bedeutet**.

- Aus diesem Grund wird bei RCTs häufig^[Derartige Analysen finden sich v.a. bei klinisch-psychologischen und biomedizinischen Trials.] auch die **\alert{klinische Signifikanz}** der Ergebnisse beurteilt, sowie der Anteil der Teilnehmenden, bei denen eine **\alert{reliable Veränderung}** stattgefunden hat.





## Klinische Signifikante & Reliable Veränderung (II) {.t}

**\alert{Klinisch Signifikante (CS) Veränderung}** \scriptsize [@jacobson1992clinical; @zahra2010reliable; @wise2004methods; @kroenke2001phq] \par \normalsize


\begin{multicols}{2}
  \footnotesize
Eine klinisch signifikante Veränderung kann als \textbf{"Behandlungserfolg"} definiert werden. Dieser bemisst sich am Unterschreiten eines \textbf{klinisch oder praktisch relevanten Cut-offs}.

Dieser Cut-off kann je nach Instrument und Untersuchungsgegenstand \textbf{variieren} (z.B. Scores $\leq$ 9 und 50\% Symptomreduktion beim PHQ-9; Kroenke, 2001). 

Die zugrundeliegende Idee ist, dass Personen damit von einer \textbf{"dysfunktionalen"} in eine \textbf{"funktionale" Population übergewechselt} sind.
  \par \normalsize
  \par \normalsize
\columnbreak
  \begin{center}
  \includegraphics[width=.5\textwidth]{assets/rci1.png}
  \newline \scriptsize  aus Jacobson \& Truax (1992) 
  \end{center}
  \par \normalsize
\end{multicols}

## Klinische Signifikante & Reliable Veränderung (II) {.t}

**\alert{Reliable Veränderung (RC)}** \scriptsize [@jacobson1992clinical; @zahra2010reliable; @wise2004methods; @kroenke2001phq] \par \normalsize

- Werden bei einer Person $i$ zu Baseline und Post-Zeitpunkt **unterschiedliche Werte** für das primäre Outcome $y$ gemessen (z.B. $y_{1,i} > y_{0,i}$) ist unklar, ob dies eine **tatsächliche Verbesserung/Verschlechterung darstellt**!

- Die Veränderung könnte u.A. **zufälliges "Rauschen"** darstellen, oder durch den **Messfehler des Instruments** verursacht sein.

- Es muss daher ein **_reliables_** Veränderungsmaß berechnet werden. Dafür wird häufig der **\alert{"Reliable Change Index"}** [@jacobson1992clinical; @jacobson1984psychotherapy] herangezogen.

## Klinische Signifikante & Reliable Veränderung (III) {.t}

\footnotesize

Es seien $y_{1,i}$ und $y_{0,i}$ die Werte des primären Outcomes einer Person $i$ zum Post-test und zu Baseline. **Der Reliable Change Index (RCI) ist dann definiert als**:

$$\text{RCI}=\frac{y_{1,i}-y_{0,i}}{s_{\text{diff}}} ~~~ \text{mit} ~~~ s_{\text{diff}} = \sqrt{2(s_0\sqrt{1-r_{yy}})^2}$$
$s_0$ ist typischerweise die **Standardabweichung von $y_0$**, und $r_{yy}$ ist die **\alert{Test-Retest-Reliabilität}** von $y$. Insgesamt repräsentiert $s_{\text{diff}}$ die **erwartete Breite** der **Verteilung der Change Scores**, wenn **keine tatsächliche Veränderung** stattgefunden hat.


Für die berechneten RCIs wird eine Standardnormalverteilung angenommen: $\text{RCI} \sim \mathcal{N}(0,1)$. Dadurch ergibt sich:

$$
\text{wenn}~
\begin{cases}
|\text{RCI}| \geq \Phi^{-1}(0.975) \Rightarrow |\text{RCI}| \geq 1.96: \text{reliable Veränderung.} \\
|\text{RCI}| < \Phi^{-1}(0.975) \Rightarrow |\text{RCI}| < 1.96: \text{\underline{keine} reliable Veränderung.}
\end{cases}
$$
\par \normalsize

## Klinische Signifikante & Reliable Veränderung (IV) {.t}

\small
Liegt sowohl klinisch signifikante als auch reliable Verbesserung vor, spricht man von **_\alert{reliable and clinically significant improvement}_** (RCSI). \par \normalsize 

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/rci2.png}
\end{center}
\scriptsize aus @zahra2010reliable. \par \normalsize


# Number Needed To Treat

## Number Needed To Treat 

\footnotesize Die **\alert{Number Needed To Treat (NNT)}** gibt an, wie viele **zusätzliche Personen** mit der untersuchten **Intervention behandelt** werden müssen, um **\underline{ein} gewünschtes Ereignis** zu erreichen [@altman1998confidence].

$\rightarrow$ Ist z.B. NNT=3, dann müssen 3 Personen die Behandlung erhalten, um bei einer Person eine reliable Verbesserungen zu bewirken (oder um bei einer Person ein negatives Outcome zu verhindern; je nach Fragestellung).

Die NNT ist definiert als die **_inverse absolute Risikoreduktion_** [ARR, @harrer2021doing]. Es sei $e$ ein gewünschtes Ereignis, z.B. reliable Verbesserung:

\begin{align*}
\openup 2\jot
\begin{split}
\text{NNT} = (p_{e_{\text{treat}}} - p_{e_{\text{control}}})^{-1} = \text{ARR}^{-1} \\
p_{e_{\text{treat}}} = \frac{n_{{e}_{\text{treat}}}}{n_{\text{treat}}} ~~~ \text{und} ~~~ p_{e_{\text{control}}} =\frac{n_{{e}_{\text{control}}}}{n_{\text{control}}}.
\end{split}
\end{align*}

Ist die NNT negativ, spricht man stattdessen von der **Number Needed to Harm** (NNH).
\small \normalsize

## $~$

\begin{tikzpicture}[remember picture,overlay]  
  \node [xshift=0cm,yshift=-0.6cm] at (current page.center)
    {\includegraphics[width=1.05\paperwidth]{assets/bg-praxis.jpg}};
\end{tikzpicture}

\Huge
\colorbox{white}{\textbf{ Praxis-Teil }} \par \normalsize 

$$~$$
$$~$$

$$~$$

$$~$$

$$~$$

# Generalisierte Lineare Modelle (GLM)

## Generalisierte Lineare Modelle (I)

\small
**Was tun, wenn der primäre Endpunkt $y$ nicht kontinuierlich & ungefähr normalverteilt ist?** \par \normalsize

```{r, message=FALSE, warning=FALSE, echo=FALSE, out.width="88%", fig.align='center'}
library(ggplot2)
library(gridExtra)

set.seed(1234)
data.frame(gamma = rgamma(10000, 2),
           poisson = rpois(10000, 3),
           binom = rbinom(10000, 1, 0.75),
           negbinom = rnbinom(1000, 1, 0.1)) -> df

ggplot(df) +
  geom_density(aes(gamma), fill = "#A1C642", color = "darkgreen", alpha = 0.8) +
  ggtitle(expression("Gammaverteilung:"~y%~%Gamma(alpha == 2, beta == 1))) +
  ylab("") +
  xlab("Einkommen, in Tausend") +
  theme_minimal() +
  theme(axis.title=element_text(size=16,face="bold"),
        plot.margin = unit(c(1,1,1,1), "cm")) -> p1

ggplot(df) +
  geom_histogram(aes(poisson), fill = "#A1C642", color = "darkgreen", 
                 alpha = 0.8, bins = 11, binwidth = 0.5) +
  ggtitle(expression("Poissonverteilung:"~y%~%Pois(lambda == 3))) +
  ylab("") +
  xlab("Anzahl an Hospitalisierungen, pro Tag") +
  theme_minimal() +
  theme(axis.title=element_text(size=16,face="bold"),
        plot.margin = unit(c(1,1,1,1), "cm"))  -> p2

ggplot(df) +
  geom_histogram(aes(binom), fill = "#A1C642", color = "darkgreen", 
                 alpha = 0.8, bins = 2, binwidth = 0.5) +
  scale_x_continuous(n.breaks = 2) +
  ggtitle(expression("Binomial-/Bernoulliverteilung:"~y%~%B(n == 1, p == 0.75))) +
  ylab("") +
  xlab("Reliable Veränderung") +
  theme_minimal() +
  theme(axis.title=element_text(size=16,face="bold"),
        plot.margin = unit(c(1,1,1,1), "cm"))  -> p3


ggplot(df) +
  geom_histogram(aes(negbinom), fill = "#A1C642", color = "darkgreen", 
                 alpha = 0.8, bins = 60, binwidth = 0.5) +
  ggtitle(expression("Negative Binomialverteilung:"~y%~%NB(r == 1, p == 0.1))) +
  ylab("") +
  xlab("Absenztage von Schülern im verg. Schuljahr") +
  theme_minimal() +
  theme(axis.title=element_text(size=15,face="bold"),
        plot.margin = unit(c(1,1,1,1), "cm"))  -> p4

grid.arrange(p1, p2, p3, p4, ncol=2)

```

## Generalisierte Linear Modelle (II)

Linear Predictor

$$\eta_i = (o_i) +\alpha + \mathbf{x}_i^\top\mathbf{\beta}$$

Kopplungsfunktion (link function):

$$g(\mu) = \eta $$


\begin{align*}
\begin{split}
g(\mu) &= \log(\mu) = \eta \\
g(\mu) &= \log\left(\frac{\mu}{E}\right) = \eta ~~~ \text{(mit Offset)}\\
&= \log(\mu) = \eta + \log(E)
\end{split}
\end{align*}


## Generalisierte Linear Modelle {.t}

\small 
**\alert{GLMs \& Link-Funktionen: Übersicht}** $~~~~~~~ ~~~~ ~ ~ ~$ \tiny $^{\text{*}}$ bekannt als "Logistische Regression". \par \normalsize


\renewcommand{\arraystretch}{1.5}
```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(kableExtra)
data.frame(
  Verteilung = c("Normal", "Binomial", "$~$", "$~$", 
                 "\\makecell[l]{Negativ\\\\Binomial}", "Poisson", "Gamma"),
  Linktyp = c("Identity", "Logit$^{\\text{*}}$", "Log", "\\makecell[l]{Complem.\\\\Log-Log}", 
              "Log", "Log", "Inverse"),
  Linkfunktion = c("$\\mu$", "$\\text{log}_e{\\left(\\frac{\\mu}{1-\\mu}\\right)}$", 
                   "$\\text{log}_e{\\left(\\mu\\right)}$", 
                   "$\\text{log}_e{\\left[-\\text{log}_e{\\left(1-\\mu\\right)}\\right]}$",
                   "$\\text{log}_e{\\left(\\frac{\\mu}{\\mu+k}\\right)}$", 
                   "$\\text{log}_e{\\left(\\mu\\right)}$", "${-\\mu}^{-1}$"),
  Support = c("$\\mathbb{R}$", "$\\frac{0,1,\\dots,m}{m}$", 
              "$\\frac{0,1,\\dots,m}{m}$", "$\\frac{0,1,\\dots,m}{m}$", 
              "$\\mathbb{N}_0$", "$\\mathbb{N}_0$", "$\\mathbb{R}_{+}$"),
  beta = c("MD", "OR", "RR", "HR", "(I)RR", "(I)RR", "-"),
  glm.family = c("\\texttt{gaussian(„identity“)}", 
                 "\\texttt{binomial(„logit“)}", 
                 "\\texttt{binomial(„log“)}", 
                 "\\texttt{binomial(„cloglog“)}", 
                 "\\text{log} (in \\texttt{MASS::glm.nb})", 
                 "\\texttt{poisson(„log“)}", 
                 "\\texttt{gamma(„inverse“)}")) -> df.tab

colnames(df.tab) = c("Verteilung", "Link-Typ", "Linkfunktion", "Support ($y$)",
                     "$\\text{exp}(\\beta_{\\text{treat}})$", "\\texttt{family}")

knitr::kable(df.tab, align="llccll", booktabs = TRUE, escape = FALSE) %>% 
  kable_styling(font_size = 7) %>% 
  row_spec(0, bold = TRUE, background = ) %>% 
  row_spec(1, hline_after = TRUE) %>% 
  row_spec(4, hline_after = TRUE) %>% 
  row_spec(5, hline_after = TRUE) %>% 
  row_spec(6, hline_after = TRUE) %>% 
  row_spec(7, hline_after = TRUE)
```

\renewcommand{\arraystretch}{1}


# Referenzen

## Referenzen {.allowframebreaks}

\scriptsize





